[
  {
    "objectID": "posts/lesson_9/index.html",
    "href": "posts/lesson_9/index.html",
    "title": "Stable Diffusion Core",
    "section": "",
    "text": "Below are some notes I took following Jeremy Howard’s wonderful lecture on Stable Diffusion Lesson 9: Deep Learning Foundations to Stable Diffusion, 2022. The lecture contains much more, but I summarized what I thought to be the core ideas / bare minimum for understanding.\nAll the included images are screenshots from the lecture (Thanks to lesson notes by rekil156. Check it for more detailed notes).\n\nThe thought process:\nSuppose we want to build a model that takes as input a random image and outputs a handwritten digit, how can we do it?\nWe know we can build models to recognize handwritten digits (input is an image of a handwritten digit and output is what the digit is eg 3 or 7). Perhaps then we can build a model (\\(f\\)) that tries to recognize the probability an image is a handwritten digit.\n\nWe can use this model to generate images of handwritten digits starting from random images. How?\nLet’s say we start with 28 x 28 pixel image - of some random pixels - as input to the model above eg X3. The model spits out the probability that X3 is a handwritten digit eg \\(P(X_3) = 0.02\\). What if we ask the model to adjust not its weights, but the pixels of the image \\(X3\\) itself, such that the \\(P(X3)\\) is handwritten digit goes up? If we do this enough times, we eventually end up with an image of a handwritten digit.\nHow can the model (\\(f\\)) know by how much it needs to change each pixel in X in order to make it a handwritten digit? Well, computing how much change is the gradient (here showing partial derivative): \\(\\frac{\\partial P(x3)}{\\partial x3}\\)\nSo, we want to train a function (nn) that gives us this gradient.\nHow do we train it?\nThis function tells us how close a “noisy” image or just “noise” is close to bring a handwritten digit. So, the model need to be able to quantify noise. It needs to know be able to go from img + noise -to-&gt; img.\n\neach input is pixels of a handwritten digit + noise from normal distribution\nAs humans we have a feeling for how much each of the images above is close to being a real handwritten digit. For a computer model (function) to do that, it’s hard to quantify this “feel” to it. The trick around this is if we make try to predict how much noise we added. The amount of noise tells us then how much of a digit it is (the more noise, the less like a digit).\n\nSo, we can generate this data.\n\nAnd train our model to predict noise (\\(n\\))\nOur model (nn) will have:\n\nInputs -noisy digits\nOutputs - noise\nloss function - MSE, between the predicted output(noise) and the actual noise\n\nAnd now we know that to turn this  to that  we need to remove this \nAnd this gives us the ability to calculate the gradient of each pixel in top image to make it look like 2nd img. The NN used to do this is a UNet\n\n\nThe problem of large image sizes:\nThe computation of the gradient for each image is quite computationally intesive. It’s a lot of compute for the \\(28 x 28\\) pixel images of handwritten digits (think one partial derivative for each pixel of a \\(28 x 28 pixel image = 784\\) done multiple times). And, we don’t want to do this only for handwritten digits in black and white. We want to do this for high quality artistic images like the \\(512 x 512 x 3\\) channel RGB images we have = \\(786,432\\) pixels.\nSolution: train on compressed images.\nCompression allows the small size while keeping the essential information in an image to retrieve it.\nHow can we compress images like this effeiciently?\nIdea: We pass images through successive convolutional layers with stride 2 (each time doubling the number of channels). At the end we add a few resnet like blocks to squish down the number of channels from 24 to 4.\n\nSo starting with \\(512x512x3\\) image, we get size of \\(64x64x4\\), we have compressed it by a factor of 48 (from \\(786432 to 16384\\) pixels).\nCompression is useful only if we can decompress (get the original image back). We can think of building the reverse architecture to do this (an inverse convolution that does the opposite).\n\nHow can we get this compression algorithm?\nThink of building this as one NN whose only function is to output the same image u gave as input.\nWhy is a model that does gives same output as input useful?\nCoz we can split it in half: the part that does the compression (encoder) and the one that decompresses (decoder).\nAnd now, if u have the decoder and the 16384 pixel image, u can get back the full 786432 pixel image.\nThese smaller compressed images are called “Latents”. We can pass them to our UNet above such that: - input: latents + noise - output : noise\nWe can subtract the output (noise) from the input (latents + noise) and get latents which we can decompress using the decoder.\nNB: ⁃ This whole encoder / decoder thing is called Autoencoder or VAE (Variational Autoencoder) ⁃ This VAE is optional. We can train on the full sized images if we’re Google and have tons of TPUs everywhere.\n\n\nWhere is my Text\nSuppose we want our handwritten digit generation model to generate a specific digit and not just anyone eg it can accept text and we tell it to generate “3” or “7” and so on. How can we do this?\nDuring training, we can pass it as input not just the img+noise but also a one-hot-encoded version of the number 3\n\nThis information (that the input image is a 3) is now avaiable for the model to make use of. It can now predict not just the noise but also that the original image is a 3 (we’re passing two things into this model, the image pixels and what digit it is in one hot encoded vector form).\nAfter training if we feed in “3”(one hot encoded) and the noise (img), it is going to say the noise is everything that doesn’t represent the number three. So this is called guidance. We can use that guidance to guide the model as to what image we want it to create.\nBut, we can’t do one-hot-encoded for everything. That would mean we need a one-hot-encoded representation of every sentence possible (infinite) eg “astronaut riding a horse”. What can we do instead?\nFor each image on the internet (where we collect data), there are usually alt tags (they will have some description of the image).\n\nWe can create two NNs / functions / models, one for the image and the other for the text. Each function spits out some random numbers (we will call them features). We then want the numbers / features for each model to be as close as possible to the numbers / features for its text and not at all like the features for other imgs & their txts. We make numbers similar via dot product\n\nWe can now think of our imgs and txt in a table. Each cell is the dot product score of img features x txt features. We want max at diagonals and small values elsewhere.\n\nThis is what we know as embeddings.\nOur loss function for this model can be defined as adding all the diagonal elements and subtracting from it the off-diagonal elements.\n\nWe can feed our text encoder with “a graceful swan”, “some beautiful swan”, “such a lovely swan” and these should all give very similar embeddings because these would all represent very similar images of swans. We’ve successfully created two models that put text and images into the same space, a multimodal(using more than one mode-images and text) model. Now, when we go back to our UNet model, we pass it these text features (which we know correspond to image features).\nThis pair of models are called : CLIP,Contrastive Language-Image Pre-training. The loss we are using is called contrastive loss.\nSo, in summary, we have:\n\nA Unet that can denoise latents into unnoisy latents\nThe decoder of VAE that can take latents and create an image\nThe CLIP text encoder which can guide the Unet with captions\n\n\nJargon:\nThe gradients that we calculate in UNet are called the score function.\nCreating noise:\nWe pick a t at random and use the correspinding sigma as noise (or beta in some papers). This is called time step."
  },
  {
    "objectID": "posts/second-post/index.html",
    "href": "posts/second-post/index.html",
    "title": "How to get blog up and running using quarto and github pages",
    "section": "",
    "text": "Anything below in italics is copied from quarto documentation(https://quarto.org/docs/publishing/github-pages.html)\n\nCreate a github repo “myblog”\nClone it to local drive git clone git@github.com:username/myblog.git\nIn the terminal, in the directory containing the cloned repo,create a new quarto blog project using: quarto create-project myblog --type website:blog\n“Simplest way to publish using GitHub Pages is to render to the docs directory and then check that directory into your repository”. We change the quarto project configurtion to use “docs” folder as the output directory by modifying the .quarto.yml file using:\n\n\nproject:\ntype: website\noutput-dir: docs\n\n\n“Add a .nojekyll file to the root of your repository that tells GitHub Pages not to do additional processing of your published site using Jekyll (the GitHub default site generation tool):”\n\nOn a Mac this is: touch .nojekyll\n\nThen render the site quarto render\n\nadd and commit all files to github\n&gt;&gt; git add .\n&gt;&gt; git commit -m “Your message”\n&gt;&gt; git push\n\n“Finally, configure your GitHub repository to publish from the docs directory of your main branch:”\n\n Img src: https://quarto.org/docs/publishing/github-pages.html\nNow, the website can be accessed at:\n\nusername.github.io/reponame/\n\nwhere username is ur github username and reponame in this example is myblog\nNote-1: Add a new post to your blog by creating a sub-directory within posts, and adding an index.qmd file to the directory. That qmd file is the new blog post and when you render that, the blog home page will automatically update to include the newest post at the top of the listing.\nNote-2: Once u add new post, run quarto render in the terminal, then add, commit and push to github to apply the changes to your blog online."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "On Brains and Machines",
    "section": "",
    "text": "Stable Diffusion Core\n\n\n\n\n\n\n\nnotes\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nAmr Aboshaisha\n\n\n\n\n\n\n  \n\n\n\n\nHow to get blog up and running using quarto and github pages\n\n\n\n\n\n\n\ncode\n\n\nhow-to\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nAmr Aboshaisha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A personal notebook for some things I find interesting."
  }
]