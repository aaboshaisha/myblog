<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Amr Aboshaisha">
<meta name="dcterms.date" content="2023-05-08">

<title>Home - Stable Diffusion Core Ideas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Stable Diffusion Core Ideas</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Amr Aboshaisha </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 8, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Below are some notes I took following Jeremy Howard’s wonderful lecture on Stable Diffusion <a href="https://www.youtube.com/watch?v=_7rMfsA24Ls&amp;list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP&amp;index=2">Lesson 9: Deep Learning Foundations to Stable Diffusion, 2022</a>. The lecture contains much more, but I summarized what I thought to be the core ideas / bare minimum for understanding.</p>
<p>All the included images are screenshots from the lecture (Thanks to lesson notes by <a href="https://rekil156.github.io/rekilblog/posts/lesson9_stableDissufion/Lesson9.html">rekil156</a>. Check it for more detailed notes).</p>
<section id="the-thought-process" class="level3">
<h3 class="anchored" data-anchor-id="the-thought-process">The thought process:</h3>
<p>Suppose we want to build a model that takes as input a random image and outputs a handwritten digit, how can we do it?</p>
<p>We know we can build models to recognize handwritten digits (input is an image of a handwritten digit and output is what the digit is eg 3 or 7). Perhaps then we can build a model (<span class="math inline">\(f\)</span>) that tries to recognize the probability an image is a handwritten digit.</p>
<p><img src="fig1.png" class="img-fluid" width="300"></p>
<p>We can use this model to generate images of handwritten digits starting from random images. How?</p>
<p>Let’s say we start with 28 x 28 pixel image - of some random pixels - as input to the model above eg X3. The model spits out the probability that X3 is a handwritten digit eg <span class="math inline">\(P(X_3) = 0.02\)</span>. What if we ask the model to adjust not its weights, but the pixels of the image <span class="math inline">\(X3\)</span> itself, such that the <span class="math inline">\(P(X3)\)</span> is handwritten digit goes up? If we do this enough times, we eventually end up with an image of a handwritten digit.</p>
<p>How can the model (<span class="math inline">\(f\)</span>) know by how much it needs to change each pixel in X in order to make it a handwritten digit? Well, computing how much change is the gradient (here showing partial derivative): <span class="math inline">\(\frac{\partial P(x3)}{\partial x3}\)</span></p>
<p>So, we want to train a function (nn) that gives us this gradient.</p>
<p>How do we train it?</p>
<p>This function tells us how close a “noisy” image or just “noise” is close to bring a handwritten digit. So, the model need to be able to quantify noise. It needs to know be able to go from img + noise -to-&gt; img.</p>
<p><img src="fig2.png" class="img-fluid" width="100"></p>
<p><em>each input is pixels of a handwritten digit + noise from normal distribution</em></p>
<p>As humans we have a feeling for how much each of the images above is close to being a real handwritten digit. For a computer model (function) to do that, it’s hard to quantify this “feel” to it. The trick around this is if we make try to predict how much noise we added. The amount of noise tells us then how much of a digit it is (the more noise, the less like a digit).</p>
<p><img src="fig3.png" class="img-fluid" width="300"></p>
<p>So, we can generate this data.</p>
<p><img src="fig4.png" class="img-fluid" width="500"></p>
<p>And train our model to predict noise (<span class="math inline">\(n\)</span>)</p>
<p>Our model (nn) will have:</p>
<ul>
<li>Inputs -noisy digits</li>
<li>Outputs - noise</li>
<li>loss function - MSE, between the predicted output(noise) and the actual noise</li>
</ul>
<p>And now we know that to turn this <img src="fig5.png" class="img-fluid" width="100"> to that <img src="fig6.png" class="img-fluid" width="80"> we need to remove this <img src="fig7.png" class="img-fluid" width="100"></p>
<p>And this gives us the ability to calculate the gradient of each pixel in top image to make it look like 2nd img. The NN used to do this is a UNet</p>
</section>
<section id="the-problem-of-large-image-sizes" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-large-image-sizes">The problem of large image sizes:</h3>
<p>The computation of the gradient for each image is quite computationally intesive. It’s a lot of compute for the <span class="math inline">\(28 x 28\)</span> pixel images of handwritten digits (think one partial derivative for each pixel of a <span class="math inline">\(28 x 28 pixel image = 784\)</span> done multiple times). And, we don’t want to do this only for handwritten digits in black and white. We want to do this for high quality artistic images like the <span class="math inline">\(512 x 512 x 3\)</span> channel RGB images we have = <span class="math inline">\(786,432\)</span> pixels.</p>
<p><strong>Solution</strong>: train on compressed images.</p>
<p>Compression allows the small size while keeping the essential information in an image to retrieve it.</p>
<p>How can we compress images like this effeiciently?</p>
<p>Idea: We pass images through successive convolutional layers with stride 2 (each time doubling the number of channels). At the end we add a few resnet like blocks to squish down the number of channels from 24 to 4.</p>
<p><img src="fig8.png" class="img-fluid" width="500"></p>
<p>So starting with <span class="math inline">\(512x512x3\)</span> image, we get size of <span class="math inline">\(64x64x4\)</span>, we have compressed it by a factor of 48 (from <span class="math inline">\(786432 to 16384\)</span> pixels).</p>
<p>Compression is useful only if we can decompress (get the original image back). We can think of building the reverse architecture to do this (an inverse convolution that does the opposite).</p>
<p><img src="fig9.png" class="img-fluid" width="500"></p>
<p>How can we get this compression algorithm?</p>
<p>Think of building this as one NN whose only function is to output the same image u gave as input.</p>
<p>Why is a model that does gives same output as input useful?</p>
<p>Coz we can split it in half: the part that does the compression (encoder) and the one that decompresses (decoder).</p>
<p>And now, if u have the decoder and the 16384 pixel image, u can get back the full 786432 pixel image.</p>
<p>These smaller compressed images are called “Latents”. We can pass them to our UNet above such that: - input: latents + noise - output : noise</p>
<p>We can subtract the output (noise) from the input (latents + noise) and get latents which we can decompress using the decoder.</p>
<p>NB: ⁃ This whole encoder / decoder thing is called Autoencoder or VAE (Variational Autoencoder) ⁃ This VAE is optional. We can train on the full sized images if we’re Google and have tons of TPUs everywhere.</p>
</section>
<section id="where-is-my-text" class="level3">
<h3 class="anchored" data-anchor-id="where-is-my-text">Where is my Text</h3>
<p>Suppose we want our handwritten digit generation model to generate a specific digit and not just anyone eg it can accept text and we tell it to generate “3” or “7” and so on. How can we do this?</p>
<p>During training, we can pass it as input not just the img+noise but also a one-hot-encoded version of the number 3</p>
<p><img src="fig10.png" class="img-fluid" width="300"></p>
<p>This information (that the input image is a 3) is now avaiable for the model to make use of. It can now predict not just the noise but also that the original image is a 3 (we’re passing two things into this model, the image pixels and what digit it is in one hot encoded vector form).</p>
<p>After training if we feed in “3”(one hot encoded) and the noise (img), it is going to say the noise is everything that doesn’t represent the number three. So this is called guidance. We can use that guidance to guide the model as to what image we want it to create.</p>
<p>But, we can’t do one-hot-encoded for everything. That would mean we need a one-hot-encoded representation of every sentence possible (infinite) eg “astronaut riding a horse”. What can we do instead?</p>
<p>For each image on the internet (where we collect data), there are usually alt tags (they will have some description of the image).</p>
<p><img src="fig11.png" class="img-fluid" width="400"></p>
<p>We can create two NNs / functions / models, one for the image and the other for the text. Each function spits out some random numbers (we will call them features). We then want the numbers / features for each model to be as close as possible to the numbers / features for its text and not at all like the features for other imgs &amp; their txts. We make numbers similar via dot product</p>
<p><img src="fig12.png" class="img-fluid" width="300"></p>
<p>We can now think of our imgs and txt in a table. Each cell is the dot product score of img features x txt features. We want max at diagonals and small values elsewhere.</p>
<p><img src="fig13.png" class="img-fluid" width="300"></p>
<p>This is what we know as embeddings.</p>
<p>Our loss function for this model can be defined as adding all the diagonal elements and subtracting from it the off-diagonal elements.</p>
<p><img src="fig14.png" class="img-fluid" width="300"></p>
<p>We can feed our text encoder with “a graceful swan”, “some beautiful swan”, “such a lovely swan” and these should all give very similar embeddings because these would all represent very similar images of swans. We’ve successfully created two models that put text and images into the same space, a multimodal(using more than one mode-images and text) model. Now, when we go back to our UNet model, we pass it these text features (which we know correspond to image features).</p>
<p>This pair of models are called : CLIP,Contrastive Language-Image Pre-training. The loss we are using is called contrastive loss.</p>
<p>So, in summary, we have:</p>
<ul>
<li>A Unet that can denoise latents into unnoisy latents</li>
<li>The decoder of VAE that can take latents and create an image</li>
<li>The CLIP text encoder which can guide the Unet with captions</li>
</ul>
<section id="jargon" class="level4">
<h4 class="anchored" data-anchor-id="jargon">Jargon:</h4>
<p>The gradients that we calculate in UNet are called the score function.</p>
<p>Creating noise:</p>
<p>We pick a t at random and use the correspinding sigma as noise (or beta in some papers). This is called time step.</p>
<p><img src="fig15.png" class="img-fluid" width="300"></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>